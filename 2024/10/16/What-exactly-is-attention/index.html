<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="17哥,17g,17G,17" />
       
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>自注意力究竟是什么？ |  17哥</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
      <script src="https://use.fontawesome.com/39301eb177.js"></script>
    <link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-What-exactly-is-attention"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  自注意力究竟是什么？
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/10/16/What-exactly-is-attention/" class="article-date">
  <time datetime="2024-10-16T17:19:11.000Z" itemprop="datePublished">2024-10-16</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E5%AD%A6/">算法与数学</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">7.3k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">31 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2024/10/16/What-exactly-is-attention/1.png" alt></p>
<p>最近的 1 年多以来，一直使用 文心一言、豆包、Kimi 等大模型来帮助自己提高各种场景的效率，但是一直没有对当前大模型的底层原理做深入了解。在编写 <a href="https://wangwei1237.github.io/LLM_in_Action/">Large Language Model in Action</a> 这本书的时候，我也曾说过：</p>
<blockquote>
<p>这是一本关于大语言模型实践的书籍，而不是一本深入研究大语言模型的运行原理和底层算法的书籍。</p>
</blockquote>
<p>但是，2024 年 10 月 1 日，OpenAI 发布了 <a target="_blank" rel="noopener" href="https://openai.com/index/api-prompt-caching/">Prompt Caching in the API</a> 以提升大语言模型 API 的性能。当听到这个消息的时候，我感到非常震惊，也非常兴奋，于是接下来的几天我总想搞明白这背后的原理是什么，这里的 <code>prompt caching</code> 又究竟是什么？</p>
<p>于是，我想，是时候需要深入了解一下当前大模型的起点——Transformer 模型，也是时候需要深入了解一下究竟什么是自注意力机制。</p>
<span id="more"></span>
<h2 id="向量的点积和矩阵表示">向量的点积和矩阵表示</h2>
<p>令 $\mathbf{x}$ 和 $\mathbf{y}$ 表示维度为 $n$ 的向量，即：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{x} &amp;= [x_1, x_2, …, x_n] \<br>
\mathbf{y} &amp;= [y_1, y_2, …, y_n]<br>
\end{aligned}<br>
$$</p>
<p>则 $\mathbf{x}$ 和 $\mathbf{y}$ 的点积运算（$\cdot$）可以表示这两个向量之间的相似度，向量点积越大，表明两个向量越相似。</p>
<p>$$<br>
\mathbf{x} \cdot \mathbf{y} = \sum_{i=1}^{n}{x_iy_i} = x_1y_1 + x_2y_2 +…+ x_ny_n<br>
$$</p>
<p>如果用向量 $\mathbf{x}$ 表示一个词的词向量，对于 $m$ 个词向量而言，其任意两个词向量之间的相似度可以表示为：</p>
<p>$$<br>
\mathbf{x_i} \cdot \mathbf{x_j} = \sum_{k=1}^{n}{x_{ik}y_{jk}} = x_{i1}y_{j1} + x_{i2}y_{j2} + … + x_{in}y_{jn} \quad \forall i,j = 1,2,\cdots,m<br>
$$</p>
<p>如果用 $m \times n$ 的矩阵 $\mathbf{X}$ 来表示 $m$ 个词向量，</p>
<p>$$<br>
\mathbf{X} = \begin{bmatrix}<br>
\mathbf{x_1} \<br>
\mathbf{x_2} \<br>
\vdots \<br>
\mathbf{x_m}<br>
\end{bmatrix} = \begin{bmatrix}<br>
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1n} \<br>
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2n} \<br>
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \<br>
x_{m1} &amp; x_{m2} &amp; \cdots &amp; x_{mn}<br>
\end{bmatrix}<br>
$$</p>
<p>则 $\mathbf{S} = \mathbf{X} \mathbf{X}^T$ 为 $m \times m$ 的矩阵，并且 $s_{ij} = \mathbf{x_i} \cdot \mathbf{x_j}$ 表示第 $i$ 个词向量和第 $j$ 个词向量之间的相似度。</p>
<p>$$<br>
\mathbf{X}\mathbf{X}^T = \begin{bmatrix}<br>
\mathbf{x_1} \cdot \mathbf{x_1} &amp; \mathbf{x_1} \cdot \mathbf{x_2} &amp; \cdots &amp; \mathbf{x_1} \cdot \mathbf{x_m} \<br>
\mathbf{x_2} \cdot \mathbf{x_1} &amp; \mathbf{x_2} \cdot \mathbf{x_2}  &amp; \cdots &amp; \mathbf{x_2} \cdot \mathbf{x_m} \<br>
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \<br>
\mathbf{x_m} \cdot \mathbf{x_1} &amp; \mathbf{x_m} \cdot \mathbf{x_2}  &amp; \cdots &amp; \mathbf{x_m} \cdot \mathbf{x_m}<br>
\end{bmatrix}<br>
$$</p>
<p>使用 <code>softmax()</code> 函数对矩阵 $\mathbf{S}$ 进行归一化处理得到矩阵 $\mathbf{W} = \text{softmax}\left(\mathbf{X}\mathbf{X}^T\right)$，使得 $\mathbf{W}$ 中每一行的所有元素之和都为 1，于是每一行的各个元素就可以看作是一个权重。</p>
<p>$w_{i1},w_{i2},\cdots,w_{im}$ 分别表示第 $i$ 个词向量和第 $1,2,\cdots,m$ 个词向量之间的相似度权重，用该权重分别乘以对应的词向量，于是我们得到了第 $i$ 个词的新的表达形式 $\mathbf{z_i}$，某个词向量的权重越大，表示相似度越高。</p>
<p>$$<br>
\mathbf{z_i} = \sum_{k=1}^{m}{w_{ik}\mathbf{x_k}} = w_{i1}\mathbf{x_1} + w_{i2}\mathbf{x_2} + \cdots + w_{im}\mathbf{x_m}<br>
$$</p>
<p>对于 $m$ 个词向量都执行如上的操作可以得到：<br>
$$<br>
\mathbf{Z} = \begin{bmatrix}<br>
\mathbf{z_1} \<br>
\mathbf{z_2} \<br>
\vdots \<br>
\mathbf{z_m}<br>
\end{bmatrix} = \begin{bmatrix}<br>
w_{11}\mathbf{x_1} + w_{12}\mathbf{x_2} + \cdots + w_{1m}\mathbf{x_m} \<br>
w_{21}\mathbf{x_1} + w_{22}\mathbf{x_2} + \cdots + w_{2m}\mathbf{x_m} \<br>
\vdots \<br>
w_{m1}\mathbf{x_1} + w_{m2}\mathbf{x_2} + \cdots + w_{mm}\mathbf{x_m}<br>
\end{bmatrix}<br>
$$</p>
<p>实际上，可以证明：</p>
<p>$$<br>
\mathbf{Z} = \mathbf{W}\mathbf{X} = \text{softmax}\left(\mathbf{X}\mathbf{X}^T\right)\mathbf{X}<br>
$$</p>
<p>于是，对于原始的词向量矩阵 $\mathbf{X}$ 而言，经过一些列的矩阵乘法运算，我们得到了根据相关性权重的加权词向量表示。也就是说原始的词向量 $\mathbf{x_i}$ 可以表示为所有词向量的加权表示 $\mathbf{z_i}$。所以，可以用 <code>Attention</code> 来解释一句话中不同词之间的相互关系。Transformer 模型中的 <code>Attention</code> 其实就是矩阵 $\mathbf{Z}$，所以 Transformer 可以理解输入序列中的不同的部分，并分析输入序列中不同词之间的关系，进而捕获到上下文信息。</p>
<p>了解如上介绍的 $\text{softmax}\left(\mathbf{X}\mathbf{X}^T\right)\mathbf{X}$ 背后的逻辑对于理解 Transformer 中的各个矩阵的含义至关重要，因此我们花了很大的篇幅来对其进行分析。</p>
<p>接下来，我们用一个具体的例子来展示如上的过程。</p>
<h3 id="举个例子🌰">举个例子🌰</h3>
<p>以 <code>I am good</code> 这句话为例，我们用词向量 $\mathbf{x_1}$ 表示 <code>I</code>，$\mathbf{x_2}$ 表示 <code>am</code>，$\mathbf{x_3}$ 表示 <code>good</code>，对应的词向量分别为：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{x_1} &amp;= [1, 3, 2] \<br>
\mathbf{x_2} &amp;= [1, 1, 3] \<br>
\mathbf{x_3} &amp;= [1, 2, 1]<br>
\end{aligned}<br>
$$</p>
<p>所以，我们有矩阵 $\mathbf{X}$：</p>
<p>$$<br>
\mathbf{X} = \begin{bmatrix}<br>
1 &amp; 3 &amp; 2 \<br>
1 &amp; 1 &amp; 3 \<br>
1 &amp; 2 &amp; 1<br>
\end{bmatrix}<br>
$$</p>
<p>于是，$\mathbf{W}$ 矩阵的计算过程如下：</p>
<p><img src="/2024/10/16/What-exactly-is-attention/zmatrix.png" alt></p>
<p>权重矩阵 $\mathbf{W}$ 中某一行分别与 $\mathbf{X}$ 的一列相乘，如前所述，该操作相当于对 $\mathbf{X}$ 中各行向量（不同的词向量）加权求和，得到的结果是每个词向量 $\mathbf{x_i}$ 经过加权求和之后的新表示 $\mathbf{z_i}$，而权重矩阵则是经过相似度和归一化计算而得到的。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/vec_matrix.png" alt></p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{z_{I}}    &amp;= 0.97 \cdot \mathbf{x_{I}} + 0.02 \cdot \mathbf{x_{am}} + 0.01 \cdot \mathbf{x_{good}} \<br>
\mathbf{z_{am}}   &amp;= 0.27 \cdot \mathbf{x_{I}} + 0.73 \cdot \mathbf{x_{am}} + 0.00 \cdot \mathbf{x_{good}} \<br>
\mathbf{z_{good}} &amp;= 0.90 \cdot \mathbf{x_{I}} + 0.05 \cdot \mathbf{x_{am}} + 0.05 \cdot \mathbf{x_{good}}<br>
\end{aligned}<br>
$$</p>
<h3 id="R-实现过程">R 实现过程</h3>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">softmax &lt;- <span class="keyword">function</span>(mat) &#123;</span><br><span class="line">  exp_mat &lt;- <span class="built_in">exp</span>(mat)           <span class="comment"># 对矩阵中的每个元素取指数</span></span><br><span class="line">  row_sums &lt;- rowSums(exp_mat)  <span class="comment"># 计算每行的总和</span></span><br><span class="line">  softmax_mat &lt;- sweep(exp_mat, <span class="number">1</span>, row_sums, FUN = <span class="string">&quot;/&quot;</span>)  <span class="comment"># 对每行进行归一化</span></span><br><span class="line">  <span class="built_in">return</span>(softmax_mat)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">X &lt;- matrix(<span class="built_in">c</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>), nrow = <span class="number">3</span>, byrow = <span class="literal">TRUE</span>)</span><br><span class="line">W &lt;- softmax(X %*% t(X))</span><br><span class="line">Z &lt;- W %*% X</span><br><span class="line">print(Z)</span><br><span class="line"></span><br><span class="line"><span class="comment">#=================================================================</span></span><br><span class="line"></span><br><span class="line">     [,<span class="number">1</span>]     [,<span class="number">2</span>]     [,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,]    <span class="number">1</span> <span class="number">2.957691</span> <span class="number">2.011295</span></span><br><span class="line">[<span class="number">2</span>,]    <span class="number">1</span> <span class="number">1.540148</span> <span class="number">2.722573</span></span><br><span class="line">[<span class="number">3</span>,]    <span class="number">1</span> <span class="number">2.864164</span> <span class="number">2.000000</span></span><br></pre></td></tr></table></figure>
<h3 id="引入新的矩阵">引入新的矩阵</h3>
<p>现在，我们引入一个新的矩阵 $\mathbf{Y}$，并令</p>
<p>$$<br>
\mathbf{Z} = \text{softmax}\left(\mathbf{X}\mathbf{Y}^T\right)\mathbf{Y} = \mathbf{W}\mathbf{Y}<br>
$$</p>
<p>根据之前的描述，此时的 $\mathbf{Z}$ 为 $\mathbf{X}$ 中的每一个词向量 $\mathbf{x_i}$ 在 $\mathbf{Y}$ 中的每一个词向量 $\mathbf{y_j}$ 的加权表示，即：</p>
<p>$$<br>
\mathbf{x_i} = \sum_{j=1}^{m}{w_{ij}\mathbf{y_j}}<br>
$$</p>
<p><img src="/2024/10/16/What-exactly-is-attention/w_xy.png" alt></p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{z_{I}}    &amp;= 0.95 \cdot \mathbf{y_{\text{我}}} + 0.05 \cdot \mathbf{y_{\text{很好}}} \<br>
\mathbf{z_{am}}   &amp;= 0.12 \cdot \mathbf{y_{\text{我}}} + 0.88 \cdot \mathbf{y_{\text{很好}}} \<br>
\mathbf{z_{good}} &amp;= 0.88 \cdot \mathbf{y_{\text{我}}} + 0.12 \cdot \mathbf{y_{\text{很好}}}<br>
\end{aligned}<br>
$$</p>
<p>如上的计算和权重只是为了说明原理而随机选择的数据，并不代表真实的相关性。因此，我们会看到 <code>good</code> 和 <code>我</code>（而不是 <code>很好</code>） 之间的关系最相似。</p>
<h2 id="Transformer-中的自注意力">Transformer 中的自注意力</h2>
<p>论文 <a target="_blank" rel="noopener" href="https://arxiv.org/html/1706.03762v7">Attention Is All You Need</a> 的 3.2 节对 Attention 的描述如下<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>：</p>
<blockquote>
<p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.</p>
<p>The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
</blockquote>
<p>论文中也给出了 Self-Attention 的计算公式：</p>
<p>$$<br>
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}<br>
$$</p>
<p>其中，$\mathbf{Q}$、$\mathbf{K}$ 和 $\mathbf{V}$ 都是矩阵，分别代表 <code>Query</code>、<code>Key</code> 和 <code>Value</code>，$d_k$ 是 $\mathbf{K}$ 的行向量维度。 <code>Query</code>、<code>Key</code> 和 <code>Value</code> 都是为了计算 <code>自注意力</code> 而引入的抽象的概念，它们都是对原始的输入 $\mathbf{X}$ 的线性变换。</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{Q} &amp;= \mathbf{X}\mathbf{W}^Q \<br>
\mathbf{K} &amp;= \mathbf{X}\mathbf{W}^K \<br>
\mathbf{V} &amp;= \mathbf{X}\mathbf{W}^V<br>
\end{aligned}<br>
$$</p>
<p>因为 $\mathbf{X}$ 是 $m \times n$ 的矩阵，所以如果令 $\mathbf{W}<sup>Q$、$\mathbf{W}</sup>K$、$\mathbf{W}^V$ 均是 $n \times n$ 的单位矩阵 $\mathbf{I}$，那么 $\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$ 经过线性变换（$\mathbf{I}$）后仍然是 $\mathbf{X}$，此时我们可以用 $\mathbf{X}$ 替换 $\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$，那么公式就变成了：</p>
<p>$$<br>
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{X}\mathbf{X}^T}{\sqrt{d_k}}\right)\mathbf{X}<br>
$$</p>
<p>这也就是为什么我们之前面说：Transformer 模型中的 <code>Attention</code> 其实就是对原始输入词向量的加权求和而得到的新的表示，在新的表示中，Transformer 可以理解输入序列中的不同的部分，并分析输入序列中不同词之间的关系，进而捕获到上下文信息。</p>
<p>实际上，为了增强增强模型的拟合能力，我们并不会采用单位矩阵 $\mathbf{I}$ 对矩阵 $\mathbf{X}$ 做线性变换，而是分别采用 $\mathbf{W}<sup>Q$、$\mathbf{W}</sup>K$、$\mathbf{W}^V$ 这三个可以通过大量语料训练而学习到的参数矩阵（参数矩阵可以是任何维度，但行向量个数必须和 $\mathbf{X}$ 的行向量维度一致）。</p>
<p>除此之外，在计算 <code>softmax()</code> 之前，Transformer 会用 $\sqrt{d_k}$ 对 $\mathbf{Q}\mathbf{K}^T$ 的结果进行缩放，所以论文中也把 <code>自注意力</code> 叫作：Scaled Dot-Product Attention。当 $d_k$ 很大时，$\mathbf{Q}\mathbf{K}^T$ 的向量点积会非常大，如果不进行缩放，<code>softmax()</code> 的梯度会非常小，进而导致在反向传播时，模型参数更新的速度会变慢，甚至接近停滞，影响模型的学习效率。</p>
<p>自注意力机制的整体流程如下图所示：</p>
<p><img src="/2024/10/16/What-exactly-is-attention/sattention.png" alt="自注意力机制"></p>
<p>所以，根据 $\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})$，Transformer 可以理解一句话中不同词之间的相互关系。</p>
<p>$\mathbf{Q},\mathbf{K},\mathbf{V}$ 和原始输入之间的关系如下图所示<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>：</p>
<p><img src="/2024/10/16/What-exactly-is-attention/qkv.png" alt="计算 Q、K、V 矩阵的示例"></p>
<h3 id="Scaled-的说明">Scaled 的说明</h3>
<p>对于一个 $m\times n$ 的矩阵 $\mathbf{A}$，<code>softmax()</code> 的计算如下：</p>
<p>$$<br>
\text{softmax}(\mathbf{A}<em>{ij}) = \frac{e<sup>{\mathbf{A}_{ij}}}{\sum_{k=1}</sup>{n}e^{\mathbf{A}</em>{ik}}}<br>
$$</p>
<p>$d_k$ 变大时，会存在向量点积结果过大的可能性，如果 $\mathbf{A}<em>{ij}$ 很大，则 $e^{\mathbf{A}</em>{ij}}$ 会迅速趋近于无穷大，而行向量中其余较小的 $\mathbf{A}_{ij}$ 则会变得相对较小。此时，<code>softmax()</code> 的结果会向 0 或 1 的两个极端值推移。</p>
<p>根据 <code>softmax()</code> 的梯度定义<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>：</p>
<p>$$<br>
\frac{\partial \text{softmax}(\mathbf{A}<em>{ij})}{\partial \mathbf{A}</em>{ij}} = \text{softmax}(\mathbf{A}<em>{ij})(1 - \text{softmax}(\mathbf{A}</em>{ij}))<br>
$$</p>
<p>当 <code>softmax()</code> 的输出值过于极端（输出 0 或 1）时，<code>softmax()</code> 的梯度结果将趋近于零，在反向传播时，模型参数的更新速度会非常慢，甚至接近停滞。这也就是所谓的梯度消失（vanishing gradient）问题，梯度消失会导致模型训练效率极大降低，特别是在深度神经网络中，梯度消失可能会导致网络无法有效学习。</p>
<p>为了避免梯度消失问题，可以使用 $\frac{1}{\sqrt{d_k}}$ 缩放点积结果：$\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}$。缩放之后，点积结果的数值会变小，确保 <code>softmax()</code> 的输入不会过大，避免 <code>softmax()</code> 输出过于极端的值，从而使得 <code>softmax()</code> 的梯度更加适中，进而更有利于模型的有效训练。</p>
<h3 id="Multi-Head-Attention">Multi-Head Attention</h3>
<p>回想一下前面我们介绍的 <code>I am good</code> 的例子，我们假设该结果就是按照 Transforer 中的自注意力机制计算而来，那么 <code>good</code> 的自注意力为：</p>
<p>$$<br>
\mathbf{z_{good}} = 0.90 \cdot \mathbf{v_{I}} + 0.05 \cdot \mathbf{v_{am}} + 0.05 \cdot \mathbf{v_{good}}<br>
$$</p>
<p>由此，我们可以看出，<code>good</code> 的自注意力值实际上由 <code>I</code> 的值向量来主导。</p>
<p>但是如果我们要计算下句中的 <code>it</code> 的自注意力值呢？<code>it</code> 的自注意力值应该由 <code>dog</code> 主导还是由 <code>food</code> 主导呢？当然，我们可以很容易的看出，<code>it</code> 的自注意力值应该由 <code>dog</code> 主导，但是机器如何来判断呢？</p>
<blockquote>
<p>A dog ate the food because it was hungry.</p>
</blockquote>
<p>如果 <code>it</code> 的自注意力值的结果如下所示，那么 <code>it</code> 的自注意力值确实由 <code>dog</code> 主导：</p>
<p><img src="/2024/10/16/What-exactly-is-attention/msattention.png" alt></p>
<p>但如果 <code>it</code> 的自注意力值的结果如下所示呢？</p>
<p><img src="/2024/10/16/What-exactly-is-attention/msattention_2.png" alt></p>
<p>因此，为了确保结果的准确性，我们不能依赖一个注意力矩阵，而应该计算多个注意力矩阵，并将这多个注意力矩阵的结果整合起来，这也就是 <code>Multi-Head Attention</code> 的由来<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>。如果我们有 $h$ 个注意力矩阵，那么我们可以按如下的方式将这 $h$ 个注意力矩阵的结果整合起来得到最终的注意力矩阵：</p>
<p>$$<br>
\text{MultiHeadAttention} = \text{Concatenate}(\mathbf{Z}_1,\cdots,\mathbf{Z}_h)\mathbf{W}^O<br>
$$</p>
<h3 id="矩阵-W-O-的说明">矩阵 W^O 的说明</h3>
<p>根据 <a target="_blank" rel="noopener" href="https://arxiv.org/html/1706.03762v7">Attention Is All You Need</a> 的 第 3.2.2 节 <strong>Multi-Head Attention</strong> 的描述，对于多头注意力机制中的任何一个注意力头 $i$ 而言，其各参数矩阵的维度如下所示：</p>
<blockquote>
<p>$W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$<br>
$W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$<br>
$W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$<br>
$W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}}$</p>
</blockquote>
<p>其中，$d_{\text{model}}$ 为模型词向量的维度，$d_k$ 为 $\mathbf{W_i}<sup>Q$、$\mathbf{W_i}</sup>K$的列向量维度，$d_v$ 为 $\mathbf {W_i}^V$ 的列向量维度。</p>
<p>所以，矩阵 $\mathbf{W}^O$ 主要有两个作用：</p>
<ul>
<li>把 $h$ 个注意力头输出的拼接结果转换成单个注意力头的维度 $\mathbb{R}^{d_{\text{model}} \times d_v}$</li>
<li>把单个注意力头的维度再次转换为模型的词向量维度 $\mathbb{R}^{d_{\text{model}} \times d_{\text{model}}}$</li>
</ul>
<p>实际上，如上的操作其实是把两个线性变换操作合并到了一个矩阵 $\mathbf{W}^O$ 中。在 <a target="_blank" rel="noopener" href="https://www.3blue1brown.com/lessons/attention">Visualizing Attention, a Transformer’s Heart</a> 的第 23:08 的时候，也对此进行了详细的说明<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>,<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/wo_note.png" alt></p>
<h2 id="Transformer-中的-Positional-Encoding">Transformer 中的 Positional Encoding</h2>
<p>对于 Transformer 模型而言，为了缩短训练时间，我们会将一句话中的所有的词并行的输入到模型中。但是，这也带来了一个问题：并行输入的词向量之间丢失了相互之间的位置信息。词序信息能够帮助 Transformer 模型学习词与词之间的相互关系，因此，为了解决这个问题，Transformer 中引入了 <code>Positional Encoding</code>。</p>
<p>对于一个给定的句子，例如：<code>I am good</code>，我们首先计算每个单词的词向量表示（假定词向量的维度为 $d_{model}$）。如果 $d_{model} = 4$，那么对于 <code>I am good</code> 而言，我们就得到了 $3 \times 4$ 的输入矩阵 $\mathbf{X}$：</p>
<p><img src="/2024/10/16/What-exactly-is-attention/pe_input_x.png" alt="输入矩阵 X"></p>
<p>如果把 $\mathbf{X}$ 直接输入到 Transformer 的 Encoder 中，那么模型无法理解不同单词之间的词序。所以，需要对 $\mathbf{X}$ 增加一些表示词序信息的数据，以便模型可以正确理解句子的含义。为此，我们可以构造一个包含位置编码信息的矩阵 $\mathbf{P}$，然后把矩阵 $\mathbf{P}$ 添加到 $\mathbf{X}$ 中，即可得到包含位置信息的输入矩阵。</p>
<p>在 <a target="_blank" rel="noopener" href="https://arxiv.org/html/1706.03762v7">Attention Is All You Need</a> 的第 3.5 节 <strong>Positional Encoding</strong> 中给出了位置矩阵的计算方式：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{P}(pos, 2i) &amp;= sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) \<br>
\mathbf{P}(pos, 2i + 1) &amp;= cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)<br>
\end{aligned}<br>
$$</p>
<ul>
<li>$pos$ 表示当前的单词在句子中的位置，也就是输入矩阵的第 $pos$ 行（从 0 开始索引）；</li>
<li>$i$ 表示当前的单词在输入矩阵中的列位置，$2i$ 表示 $\mathbf{P}$ 的所有偶数列，$2i+1$ 表示所有的奇数列。</li>
<li>$d_{model}$ 是词向量的维度。</li>
</ul>
<p>根据位置矩阵的计算公式，对与所有的偶数列，使用正弦函数计算位置编码信息；对于所有的奇数列，使用余弦函数计算位置编码信息。对于 <code>I am good</code> 而言，位置矩阵 $\mathbf{P}$ 的计算结果如下：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{P} &amp;=<br>
\begin{bmatrix}<br>
sin\left(\frac{pos}{10000^{0}}\right) &amp; cos\left(\frac{pos}{10000^{0}}\right) &amp; sin\left(\frac{pos}{10000^{2/4}}\right) &amp; cos\left(\frac{pos}{10000^{2/4}}\right) \<br>
sin\left(\frac{pos}{10000^{0}}\right) &amp; cos\left(\frac{pos}{10000^{0}}\right) &amp; sin\left(\frac{pos}{10000^{2/4}}\right) &amp; cos\left(\frac{pos}{10000^{2/4}}\right) \<br>
sin\left(\frac{pos}{10000^{0}}\right) &amp; cos\left(\frac{pos}{10000^{0}}\right) &amp; sin\left(\frac{pos}{10000^{2/4}}\right) &amp; cos\left(\frac{pos}{10000^{2/4}}\right)<br>
\end{bmatrix} \<br>
&amp;= \begin{bmatrix}<br>
sin(0) &amp; cos(0) &amp; sin(0/100) &amp; cos(0 / 100) \<br>
sin(1) &amp; cos(1) &amp; sin(1/100) &amp; cos(1 / 100) \<br>
sin(2) &amp; cos(2) &amp; sin(2/100) &amp; cos(2 / 100) \<br>
\end{bmatrix} \<br>
&amp;= \begin{bmatrix}<br>
0 &amp; 1 &amp; 0 &amp; 1 \<br>
0.841 &amp; 0.54 &amp; 0.01 &amp; 0.99 \<br>
0.909 &amp; -0.416 &amp; 0.02 &amp; 0.99<br>
\end{bmatrix}<br>
\end{aligned}<br>
$$</p>
<p>于是，我们得到了包含位置信息的输入矩阵 $\mathbf{X} + \mathbf{P}$：</p>
<p>$$<br>
\begin{aligned}<br>
\mathbf{X} + \mathbf{P} &amp;=<br>
\begin{bmatrix}<br>
1.8 &amp; 2.2 &amp; 3.4 &amp; 5.8 \<br>
7.3 &amp; 9.9 &amp; 8.5 &amp; 7.1 \<br>
9.1 &amp; 7.1 &amp; 0.9 &amp; 10.1<br>
\end{bmatrix} +<br>
\begin{bmatrix}<br>
0 &amp; 1 &amp; 0 &amp; 1 \<br>
0.841 &amp; 0.54 &amp; 0.01 &amp; 0.99 \<br>
0.909 &amp; -0.416 &amp; 0.02 &amp; 0.99<br>
\end{bmatrix} \<br>
&amp;= \begin{bmatrix}<br>
1.8 &amp; 3.2 &amp; 3.4 &amp; 6.8 \<br>
8.141 &amp; 10.44 &amp; 8.51 &amp; 8.09 \<br>
10.009 &amp; 6.684 &amp; 0.92 &amp; 11.09<br>
\end{bmatrix}<br>
\end{aligned}<br>
$$</p>
<p>计算位置矩阵的 R 代码如下所示：</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 位置编码函数，从 0 开始计算</span></span><br><span class="line">get_position_encoding &lt;- <span class="keyword">function</span>(<span class="built_in">seq_len</span>, d_model) &#123;</span><br><span class="line">  position &lt;- 0:(<span class="built_in">seq_len</span> - <span class="number">1</span>)</span><br><span class="line">  div_term &lt;- <span class="built_in">exp</span>(seq(<span class="number">0</span>, d_model - <span class="number">1</span>, by = <span class="number">2</span>) * -(<span class="built_in">log</span>(<span class="number">10000</span>) / d_model))</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 初始化位置编码矩阵</span></span><br><span class="line">  position_enc &lt;- matrix(<span class="number">0</span>, nrow = <span class="built_in">seq_len</span>, ncol = d_model)</span><br><span class="line">  <span class="keyword">for</span> (pos <span class="keyword">in</span> <span class="number">0</span>:(<span class="built_in">seq_len</span> - <span class="number">1</span>)) &#123;</span><br><span class="line">    <span class="comment"># 奇数索引：sin函数</span></span><br><span class="line">    position_enc[pos + <span class="number">1</span>, seq(<span class="number">1</span>, d_model, by = <span class="number">2</span>)] &lt;- <span class="built_in">sin</span>(pos * div_term)</span><br><span class="line">    <span class="comment"># 偶数索引：cos函数</span></span><br><span class="line">    position_enc[pos + <span class="number">1</span>, seq(<span class="number">2</span>, d_model, by = <span class="number">2</span>)] &lt;- <span class="built_in">cos</span>(pos * div_term)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">return</span>(position_enc)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成位置编码矩阵</span></span><br><span class="line">position_encoding_matrix &lt;- get_position_encoding(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看位置编码矩阵</span></span><br><span class="line">print(position_encoding_matrix)</span><br></pre></td></tr></table></figure>
<p>如果一个句子包含 128 个词，每个词向量的维度 $d_{model} = 512$，那么位置编码矩阵的维度为 $128 \times 512$，可以用如下的 R 代码生成位置编码矩阵，并对其可视化。</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">position_encoding_matrix &lt;- get_position_encoding(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line">position_encoding_df &lt;- melt(position_encoding_matrix)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制热图</span></span><br><span class="line">ggplot(position_encoding_df, aes(x = Var2, y = Var1, fill = value)) +</span><br><span class="line">  geom_tile() +</span><br><span class="line">  scale_fill_viridis_c() +                     <span class="comment"># 使用类似的颜色映射</span></span><br><span class="line">  scale_y_reverse() + </span><br><span class="line">  theme_minimal(base_family = <span class="string">&quot;STKaiti&quot;</span>) + </span><br><span class="line">  scale_x_continuous(expand = expansion(mult = <span class="built_in">c</span>(<span class="number">0</span>, <span class="number">0.05</span>))) + </span><br><span class="line">  labs(title = <span class="string">&quot;Position Encoding 可视化&quot;</span>,</span><br><span class="line">       x = <span class="string">&quot;Dimensions&quot;</span>, y = <span class="string">&quot;words&quot;</span>, fill = <span class="string">&quot;Value&quot;</span>) + </span><br><span class="line">  theme(panel.grid.major.x = element_blank(),  <span class="comment"># 移除 x 轴的主要网格线</span></span><br><span class="line">        panel.grid.minor.x = element_blank(),</span><br><span class="line">        panel.grid.minor.y = element_blank(),</span><br><span class="line">        axis.line = element_line(),           <span class="comment"># 保留x轴和y轴本身</span></span><br><span class="line">        axis.ticks = element_line(),          <span class="comment"># 保留x轴和y轴上的刻度线</span></span><br><span class="line">        legend.position = <span class="string">&quot;top&quot;</span>,              <span class="comment"># 图例放置在左上角</span></span><br><span class="line">        legend.justification = <span class="built_in">c</span>(<span class="number">0</span>, <span class="number">1</span>),       <span class="comment"># 设置图例的对齐方式</span></span><br><span class="line">        legend.title = element_blank())</span><br></pre></td></tr></table></figure>
<p><img src="/2024/10/16/What-exactly-is-attention/pe_hot_image.png" alt></p>
<h2 id="Transformer-中的-Encoder">Transformer 中的 Encoder</h2>
<p>在 <code>Multi-Head Attention</code> 和 <code>Positional Encoding</code> 的基础之上，再增加前馈网络、叠加和归一组件，就得到了完整的 Transformer Encoder。当然，实际中，可以将 $N$ 个 Encoder 一个一个的叠加起来，最后一个 Encoder 的输出就是原始输入内容的特征值 $\mathbf{R}$（矩阵 $\mathbf{R}$ 与输入内容的词向量矩阵的维度是一致的）。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/encoder.png" alt="Encoder 架构图"></p>
<h2 id="Transformer-中的-Decoder">Transformer 中的 Decoder</h2>
<p>假设我们要把英语句子 <code>I am a student</code>（原句）翻译成中文 <code>我是一个学生</code>（目标句）。</p>
<ol>
<li>首先，我们把原句 <code>I am a student</code> 输入到 Transformer 的 Encoder 中，让 Encoder 学习原句并得到原句的特征矩阵 $\mathbf{R}$。</li>
<li>然后，我们把 $\mathbf{R}$ 输入到解码器，解码器根据 $\mathbf{R}$ 和解码器之前的输出作为输入，并生成目标句中的下一个词，直到生成目标句为止（解码器每次迭代会生成一个词，直到生成目标句为止）。</li>
</ol>
<p>具体的生成过程如下图所示：</p>
<p><img src="/2024/10/16/What-exactly-is-attention/decode_demo1.gif" alt="解码器的例子"></p>
<h3 id="带掩码的多头注意力层">带掩码的多头注意力层</h3>
<p>以如上的翻译认为为例，假设训练数据集如下所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">原句</th>
<th style="text-align:center">目标句</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">I am a student</td>
<td style="text-align:center">我是一个学生</td>
</tr>
<tr>
<td style="text-align:center">I am a teacher</td>
<td style="text-align:center">我是一个老师</td>
</tr>
<tr>
<td style="text-align:center">Good morning</td>
<td style="text-align:center">早上好</td>
</tr>
<tr>
<td style="text-align:center">Thank you very much</td>
<td style="text-align:center">非常感谢你</td>
</tr>
</tbody>
</table>
<p>上表所示的数据集由两部分构成：<code>原句</code> 和 <code>目标句</code>。如前所述，我们也了解了解码器在 <strong>运行时</strong> 逐字（token）预测目标句的过程。但是，在训练时，由于我们有标注好的、正确的 <code>目标句</code>，所以，为了简化训练过程，我们可以对整个 <code>目标句</code> 稍作修改然后将其作为输入（而不需要像 <strong>运行时</strong> 那样逐字输入）。解码器把输入的 <strong>&lt;sos&gt;</strong>（start of sequence）作为第一个 token，并在之后的每一步中把下一个预测 token 追加到当前的输入，以预测目标句，直到预测输出为 <strong>&lt;eos&gt;</strong>（end of sequence）为止。因此，在训练时，我们只需要把 <strong>&lt;sos&gt;</strong> 添加到 <code>目标句</code> 的开头，然后将其作为输入发送给解码器，具体如下图所示。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/encoder_decoder_demo1.png" alt="Transformer 中的编码器和解码器"></p>
<p>如上图所示，如果要把 <code>I am a student</code> 翻译成 <code>我是一个学生</code>，我们只需要把 <code>&lt;sos&gt;我是一个学生</code> 作为输入发送给解码器，解码器的目标是根据 <code>I am a student</code> 和 <code>&lt;sos&gt;我是一个学生</code> 预测输出 <code>我是一个学生&lt;eos&gt;</code>。</p>
<p>对于解码器的输入序列而言，我们会先将其转换成嵌入矩阵并增加位置编码信息得到输入矩阵 $\mathbf{X}$，然后我们将 $\mathbf{X}$ 输入到编码器中进行后续的预测。假设矩阵 $\mathbf{X}$ 为：</p>
<p>$$<br>
\mathbf{X} = \begin{bmatrix}<br>
1.1 &amp; 1.2  &amp; 1.3  &amp; \cdots &amp; 1.4 \<br>
1.5 &amp; 1.6  &amp; 1.7  &amp; \cdots &amp; 1.8 \<br>
1.2 &amp; 1.1  &amp; 1.3  &amp; \cdots &amp; 1.2 \<br>
1.4 &amp; 1.5  &amp; 1.6  &amp; \cdots &amp; 1.7 \<br>
1.6 &amp; 1.3  &amp; 1.4  &amp; \cdots &amp; 1.5<br>
\end{bmatrix}<br>
$$</p>
<p>解码器首先会计算 $\mathbf{X}$ 的自注意力矩阵 $\mathbf{Z}$，然后计算句子中所有词之间的关联并提取每个词的信息。但是与编码器不同的是，在解码器的 <strong>运行时</strong>，解码器只是将上一个步骤生成的词作为输入，而不是将整个输入序列作为输入。因此，在第 $2$ 步时，解码器的输入中只有 $[<sos>, 我]$，所以我们只能得到这两个词之间的关联，而不能得到之后的任何词的信息。</sos></p>
<p>因此，在训练期间，虽然我们可以将整个 <code>目标句</code> 作为输入，但是我们仍然需要以解码器在 <code>运行时</code> 的运行方式来训练模型。也就是说，在训练期间，我们只能计算 <code>目标句</code> 序列中某个词和该词之前的所有词之间的注意力。为此，可以引入 <code>掩码</code> 的概念以实现这个目的。例如，我们想要预测 <code>&lt;sos&gt;我</code> 之后的词 <code>是</code>，此时，模型应该只看到 <code>&lt;sos&gt;我</code>，所以我们需要用掩码来阻止模型看到 <code>我</code> 之后的词。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/mask_demo.png" alt="掩码的例子"></p>
<p>根据注意力矩阵的计算方式：</p>
<p>$$<br>
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}<br>
$$</p>
<p>我们在计算 <code>softmax()</code> 之前，对 $\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}$ 矩阵进行如下的操作即可：在预测第 $i$ 个词的注意力时，用 $-\infty$ 替换 $\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}$ 中该词之后的所有点积结果。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/mask_demo_2.png" alt="带掩码的权重矩阵"></p>
<p>此时，我们就可以利用 <code>softmax()</code> 对如上的掩码矩阵进行计算，并乘以对应的矩阵 $\mathbf{V}$，得到最终的注意力矩阵 $\mathbf{Z}$。同样的，如果我们有 $h$ 个注意力矩阵，那么我们可以按如下的方式将这 $h$ 个注意力矩阵的结果整合起来得到最终的注意力矩阵 $\mathbf{M}$：</p>
<p>$$<br>
\mathbf{M} = \text{Concatenate}(\mathbf{Z}_1,\cdots,\mathbf{Z}_h)\mathbf{W}^O<br>
$$</p>
<h3 id="多头注意力层">多头注意力层</h3>
<p>为了根据解码器中计算的 <code>原句</code> 的注意力矩阵 $\mathbf{R}$ 和解码器中计算的 <code>目标句</code> 的带掩码的注意力矩阵 $\mathbf{M}$ 来预测最终的 <code>目标句</code>，我们还需要一个多头注意力层。由于该层涉及到了编码器与解码器的交互，因此，这一层也称之为 <strong>编码器-解码器注意力层</strong>（Encoder-Decoder Attention）。</p>
<p>多头注意力机制的第一步就是要创建查询矩阵 $\mathbf{Q}$、键矩阵 $\mathbf{K}$ 和值矩阵 $\mathbf{V}$。如前所述，可以通过输入矩阵 $\mathbf{X}$ 乘以权重矩阵 $\mathbf{W}^Q$、键矩阵 $\mathbf{W}^K$ 和值矩阵 $\mathbf{W}^V$ 得到查询矩阵 $\mathbf{Q}$、键矩阵 $\mathbf{K}$ 和值矩阵 $\mathbf{V}$。但是现在，我们有两个输入矩阵 $\mathbf{R}$（编码器输出的特征矩阵） 和 $\mathbf{M}$（解码器中带掩码的多头注意力层输出的注意力矩阵），我们该如何使用这两个输入矩阵呢？怎么利用这两个输入矩阵生成查询矩阵 $\mathbf{Q}$、键矩阵 $\mathbf{K}$ 和值矩阵 $\mathbf{V}$ 呢？</p>
<p>我们使用矩阵 $\mathbf{M}$ 生成查询矩阵 $\mathbf{Q}$，以使得查询矩阵可以包含 <code>目标句</code> 的特征；使用矩阵 $\mathbf{R}$ 生成键矩阵 $\mathbf{K}$ 和值矩阵 $\mathbf{V}$，以使得键矩阵和值矩阵可以包含 <code>原句</code> 的特征。然后再利用计算求得的 $\mathbf{Q}$、$\mathbf{K}$、$\mathbf{V}$ 计算注意力矩阵 $\mathbf{Z}$。具体的步骤如下所示：</p>
<ol>
<li>
<p>使用矩阵 $\mathbf{M}$ 乘以 $\mathbf{W}^Q$ 生成查询矩阵 $\mathbf{Q}$，使用矩阵 $\mathbf{R}$ 乘以 $\mathbf{W}^K$ 生成键矩阵 $\mathbf{K}$，使用矩阵 $\mathbf{R}$ 乘以 $\mathbf{W}^V$ 生成值矩阵 $\mathbf{V}$。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/m_r_qkv.png" alt="根据矩阵 M 和矩阵 R 创建矩阵 Q、K、V"></p>
</li>
<li>
<p>计算查询矩阵 $\mathbf{Q}$ 和键矩阵 $\mathbf{K}^T$ 的点积，得到 <code>目标句</code> 中每个词和 <code>原句</code> 中每个词的相似度。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/m_r_q_mul_k.png" alt="查询矩阵和键矩阵的点积"></p>
</li>
<li>
<p>根据 $\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}$ 的到权重矩阵，为了演示方便，此处我们令 $d_k = 64$。</p>
</li>
<li>
<p>根据 $\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right) \mathbf{V}$ 计算注意力矩阵 $\mathbf{Z}$。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/m_r_attention.png" alt></p>
<p>如图所示：</p>
<p>$$<br>
\mathbf{z}<em>{我} = 0.99 \cdot \mathbf{v}</em>{I} + 0.01 \cdot \mathbf{v}<em>{am} + 0.0 \cdot \mathbf{v}</em>{a} + 0.0 \cdot \mathbf{v}_{student}<br>
$$</p>
<p>由此，如上的结果可以让模型理解 <code>目标句</code> 中的 <code>我</code> 指代的其实就是 <code>原句</code> 中的 <code>I</code>。</p>
</li>
</ol>
<p>同样的，在该层中，我们仍然可以有 $h$ 个注意力矩阵，那么我们可以按如下的方式将这 $h$ 个注意力矩阵的结果整合起来得到最终的注意力矩阵 $\mathbf{Decoder Multi-head Attention}$：</p>
<p>$$<br>
\mathbf{Decoder Multi-head Attention} = \text{Concatenate}(\mathbf{Z}_1,\cdots,\mathbf{Z}_h)\mathbf{W}^O<br>
$$</p>
<p>同编码器一样，在多头注意力层之后，我们还会增加叠加和归一组件、前馈网络层并最终构成了完整的解码器。在实际应用中，我们可以将 $N$ 个解码器一个一个的叠加起来，最后一个解码器的输出就是最终的 <code>目标句</code> 的特征值 $\mathbf{T}$。</p>
<p><img src="/2024/10/16/What-exactly-is-attention/encoder_decoder.png" alt="Decoder 架构图"></p>
<h3 id="线性层和-softmax-层">线性层和 softmax 层</h3>
<p>一旦解码器学习到了 <code>目标句</code> 的特征值 $\mathbf{T}$，我们就可以使用线性层和 <code>softmax</code> 层来生成最终的 <code>目标句</code>。线性层将生成一个 <code>logit</code> 向量，其大小等于 <code>目标句</code> 中的词汇量。假设目标句的词汇量只有 4 个词组成：</p>
<p>$$<br>
Vocabulary = { 是, 我, 学生, 一个 }<br>
$$</p>
<p>那么，线性层返回的 <code>logit</code> 的向量的维度将是 4。然后使用 <code>softmax()</code> 将 <code>logit</code> 向量转换成概率表示，最后输出具有最高概率值的词的索引值。</p>
<p>例如，如果解码器的输入是 <code>&lt;sos&gt;我</code>，则解码器需要根据当前的输入预测下一个词。我们把最顶层的解码器的输出特征值 $\mathbf{T}$ 输入到线性层，假设我们得到的 <code>logit</code> 向量如下所示：</p>
<p>$$<br>
\text{logit} = [39, 20, 31, 35]<br>
$$</p>
<p>然后，我们对如上的 <code>logit</code> 向量应用 <code>softmax()</code> 计算，并得到如下的概率向量：</p>
<p>$$<br>
\text{probability} = [0.98, 0.00, 0.00, 0.12]<br>
$$</p>
<p>从中，我们可以看出，索引值为 0 的词 <code>是</code> 具有最高的概率值，因此，解码器会输出索引值为 0 的词 <code>是</code> 作为下一个词。通过该方式，解码器可以不断的预测 <code>目标句</code> 中的下一个词，直到输出 <code>&lt;eos&gt;</code> 为止。</p>
<h2 id="Transformer-整体架构">Transformer 整体架构</h2>
<p>根据如上的介绍，我们可以得到 Transformer 模型的整体架构图：</p>
<p><img src="/2024/10/16/What-exactly-is-attention/transformer.png" alt="Transformer 的架构图"></p>
<p>在上图中，$N \times$ 表示可以堆叠 $N$ 个编码器和解码器，$h \times$ 表示可以堆叠 $h$ 个注意力矩阵。我们可以看到，一旦输入 <code>原句</code>，编码器就会学习 <code>原句</code> 的特征并将特征发送给解码器，而解码器则会不断的预测并生成 <code>目标句</code>。</p>
<p>在内容生成的场景中，我们提供给模型的 <code>prompt</code> 就相当于 <code>原句</code>，而 <code>目标句</code> 则相当于生成的内容。从 Transformer 的整体架构图中，我们可以看到，模型会不断的根据 <code>prompt</code> 和 <code>目标句</code> 的特征来生成最终的 <code>目标句</code>，而不会生成与 <code>prompt</code> 无关的内容，这也就是我们所说的 <strong>上下文学习</strong>（ICL: in context learning） 和 <strong>指令遵循</strong>（instruction following）。</p>
<h2 id="参考文献">参考文献</h2>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://arxiv.org/html/1706.03762v7">Attention Is All You Need</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">The Softmax function and its derivative</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://medium.com/@ngiengkianyew/multi-headed-attention-8b940b76c351">Intuition for Multi-headed Attention</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://www.3blue1brown.com/lessons/attention">Visualizing Attention, a Transformer’s Heart</a> <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1TZ421j7Ke?spm_id_from=333.788.recommend_more_video.-1&amp;vd_source=fbeb46d16d08ad900fac814e55c3f27f">Visualizing Attention, a Transformer’s Heart（B站版）</a> <a href="#fnref6" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          打赏
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>版权声明： </strong>
          
          本博客所有文章除特别声明外，著作权归作者所有。转载请注明出处！
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://wangwei1237.github.io/2024/10/16/What-exactly-is-attention/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="tag">自注意力机制</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2024/10/31/From-Transformer-To-GPT/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            从 Transformer 到 GPT
          
        </div>
      </a>
    
    
      <a href="/2024/10/06/The-Statistical-Approach-for-the-Product-Evaluation/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">推断统计方法在评估分析中的应用</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "ppRS6IT7xMHmCl54L7ynIC2Z-gzGzoHsz",
    app_key: "qEmM49ZlU6LOwXCHjzMUECKu",
    path: window.location.pathname,
    avatar: "mp",
    placeholder: "快来评论吧~",
    recordIP: true,
    visitor: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2024
        <i class="ri-heart-fill heart_icon"></i> wangwei
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <!--<span id="busuanzi_container_page_pv">本文阅读量<span id="busuanzi_value_page_pv_"></span>次</span>
  <span class="division">|</span>
  -->
  <span id="busuanzi_container_site_uv"><i class="ri-user-3-fill"></i>本站访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span id="busuanzi_container_site_pv"><i class="ri-eye-fill"></i>本站浏览次数:<span id="busuanzi_value_site_pv"></span></span>
</span>
<script>
  
</script>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="17哥"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/books">图书</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/shares">分享</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/aboutme">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/weixin.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->
 
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script>
        
            <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"></script>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css">
        
    
 
<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>