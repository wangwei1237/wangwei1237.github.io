<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="17哥,17g,17G,17" />
       
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> 17哥</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <script src="https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.min.js"></script>
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
      <script src="https://use.fontawesome.com/39301eb177.js"></script>
    <link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      



<!-- Subtitle -->

<div id="main">
  <section class="outer">
  
  <ul class="ads">
    
        <li>
            <a href="https://wangwei1237.github.io/introduction_to_probability_and_statistics/">
                <img src="https://wangwei1237.github.io/introduction_to_probability_and_statistics/cover.png" width="150" alt="Introduction to Probability and Statistics for Engineers and Scientists">
            </a>
        </li>
    
        <li>
            <a href="https://wangwei1237.github.io/LLM_in_Action/">
                <img src="https://wangwei1237.github.io/LLM_in_Action/cover.png" width="150" alt="大模型实践">
            </a>
        </li>
    
        <li>
            <a target="_blank" rel="noopener" href="https://www.oreilly.com/library/view/building-microservices-2nd/9781492034018/">
                <img src="/books/index/building_microservices_2ed.png" width="150" alt="Building Microservices, 2nd Edition">
            </a>
        </li>
    
        <li>
            <a href="https://wangwei1237.github.io/monolith-to-microservices/">
                <img src="https://wangwei1237.github.io/2020/08/19/monolith-to-microservices/cover.png" width="150" alt="从单体应用到微服务架构（中文版）">
            </a>
        </li>
    
</ul>
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-Deploy-vui-audio-model"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2025/06/14/Deploy-vui-audio-model/"
    >在阿里云上部署 Vui 语音模型</a> 
</h2>
 

      
    <div class="article-author" >
        作者：
            王伟 ,
         马海亮
    </div>


    </header>
     
    <div class="article-meta">
      <a href="/2025/06/14/Deploy-vui-audio-model/" class="article-date">
  <time datetime="2025-06-14T10:32:41.000Z" itemprop="datePublished">2025-06-14</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2025/06/14/Deploy-vui-audio-model/tts-basics.jpg" alt></p>
<p>6 月初，Fluxions-AI 团队在 GitHub 上开源了一款轻量级、可在设备端运行的语音对话模型：<a target="_blank" rel="noopener" href="https://github.com/fluxions-ai/vui">Vui</a>。Vui 语音模型基于 <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/en/model_doc/llama">Llama transformer</a> 架构来预测下一个语音 token。据 Fluxions-AI 团队介绍，他们在 <a target="_blank" rel="noopener" href="https://x.com/harrycblum/status/1752698806184063153">2 张 4090</a> 显卡上完成了 Vui 的训练，并且提供了 3 个不同的模型。Vui训练成本极低，参数量也较小，可以支持语气词的拟人化模拟，在生成效果上更自然、更逼真，并且还支持两人对话的语音生成，非常适合语聊、生成播客语音内容、采访/访谈配音等场景。</p> 
      <a class="article-more-link" href="/2025/06/14/Deploy-vui-audio-model/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DSW/" rel="tag">DSW</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TTS/" rel="tag">TTS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Vui/" rel="tag">Vui</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Will-Natural-Language-Unify-All-Industries-and-Fields"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2025/06/01/Will-Natural-Language-Unify-All-Industries-and-Fields/"
    >自然语言是否会统一所有的行业和领域?</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/06/01/Will-Natural-Language-Unify-All-Industries-and-Fields/" class="article-date">
  <time datetime="2025-06-01T12:00:00.000Z" itemprop="datePublished">2025-06-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2025/06/01/Will-Natural-Language-Unify-All-Industries-and-Fields/1.jpg" alt></p>
<p>2025 年，大语言模型的发展进入爆发期，以 ChatGPT、Claude、Gemini、Deepseek 等为代表的大型语言模型（LLM）在文本、推理、代码、图片、视频等领域取得飞跃式的发展。在教育、医疗、工业制造等不同的行业中，大模型也有了越来越多的实际用例，展现出解决复杂问题的能力。</p>
<p>看起来，大模型正在向着通用人工智能（AGI）的方向发展，能够跨越多个领域而形成通用认知能力，跳出预先训练的狭窄任务范围，胜任任何智力任务（数学证明、科学发现、日常对话……），最终像人类一样理解、学习和解决不同领域的问题。</p>
<p>Prompt 作为与大模型交互的主要方式，其本质上还是用自然语言的方式与大模型交互。那么，未来，自然语言是否会统一所有行业和领域呢？编程语言、数学语言、化学语言、镜头语言、音乐语言……是否都会被自然语言所取代？</p> 
      <a class="article-more-link" href="/2025/06/01/Will-Natural-Language-Unify-All-Industries-and-Fields/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AGI/" rel="tag">AGI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%80%9D%E8%80%83/" rel="tag">思考</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Quickly-Deploy-the-LLM-Model-Using-Aliyun-PAI-DSW"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2025/05/04/Quickly-Deploy-the-LLM-Model-Using-Aliyun-PAI-DSW/"
    >使用阿里云PAI-DSW快速部署模型</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/05/04/Quickly-Deploy-the-LLM-Model-Using-Aliyun-PAI-DSW/" class="article-date">
  <time datetime="2025-05-04T16:43:52.000Z" itemprop="datePublished">2025-05-04</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2025/05/04/Quickly-Deploy-the-LLM-Model-Using-Aliyun-PAI-DSW/1.png" alt></p>
<p>自从 2025 年 1 月 20 日，DeepSeek-R1 发布以来，大模型行业经历了 DeepSeek 的火爆、经历了大模型厂商的密集迭代和发布、更经历了开源大模型生态的快速发展和壮大。</p>
<p>尤其是，开源大模型厂商会同时发布多种不同参数量的模型以满足不同的应用场景，例如 2025 年 1 月 26 日，阿里发布的 <a target="_blank" rel="noopener" href="https://qwenlm.github.io/blog/qwen2.5-vl/">Qwen2.5-VL 系列模型</a> 就同时提供了 3B、7B、72B 三个不同参数量的版本。以 <a target="_blank" rel="noopener" href="https://modelscope.cn/models/Qwen/Qwen2.5-VL-3B-Instruct/summary">Qwen2.5-VL-3B-Instruct</a> 为例，其模型文件的大小仅为 7GB 左右，这使得我们可以类似 <a target="_blank" rel="noopener" href="https://www.nvidia.cn/geforce/graphics-cards/40-series/rtx-4080-family/">RTX 4080</a> 这样的消费级显卡上去部署 Qwen2.5-VL-3B，以体验其模型能力。</p>
<p>但是，如果连 <a target="_blank" rel="noopener" href="https://www.nvidia.cn/geforce/graphics-cards/40-series/rtx-4080-family/">RTX 4080</a> 这样的消费级显卡都没有，或者说我们并不想购买一张显卡来部署模型，那么我们又如何来部署模型呢？另外，虽然 Qwen2.5-VL-3B 的模型文件大小仅为 7GB 左右，但是如果我们的网络带宽比较低的时候，下载模型文件也会是一个比较耗时的过程。</p> 
      <a class="article-more-link" href="/2025/05/04/Quickly-Deploy-the-LLM-Model-Using-Aliyun-PAI-DSW/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPU/" rel="tag">GPU</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PAI-DSW/" rel="tag">PAI-DSW</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" rel="tag">模型部署</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Unveiling-the-Mystery-of-MCP-with-Examples"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2025/04/17/Unveiling-the-Mystery-of-MCP-with-Examples/"
    >用示例揭开 MCP 的神秘面纱</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/04/17/Unveiling-the-Mystery-of-MCP-with-Examples/" class="article-date">
  <time datetime="2025-04-17T09:30:10.000Z" itemprop="datePublished">2025-04-17</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2025/04/17/Unveiling-the-Mystery-of-MCP-with-Examples/1.jpg" alt></p>
<p>2024 年 11 月 25 日，Anthropic 在官方博客 <a target="_blank" rel="noopener" href="https://www.anthropic.com/news/model-context-protocol">Introducing the Model Context Protocol</a> 中正式开源了可以将各种工具链接到 LLM 的 MCP（<em>Model Context Protocol</em>）协议，以增强 LLM 的能力。</p>
<blockquote>
<p>Today, we’re open-sourcing the Model Context Protocol (MCP), a new standard for connecting AI assistants to the systems where data lives, including content repositories, business tools, and development environments. Its aim is to help frontier models produce better, more relevant responses.</p>
</blockquote> 
      <a class="article-more-link" href="/2025/04/17/Unveiling-the-Mystery-of-MCP-with-Examples/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Agent/" rel="tag">Agent</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MCP/" rel="tag">MCP</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Seven-Principles-for-Large-Model-Evaluation"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2025/03/29/Seven-Principles-for-Large-Model-Evaluation/"
    >大模型评测 7 条原则</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/03/29/Seven-Principles-for-Large-Model-Evaluation/" class="article-date">
  <time datetime="2025-03-29T10:35:46.000Z" itemprop="datePublished">2025-03-29</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2025/03/29/Seven-Principles-for-Large-Model-Evaluation/1.png" alt></p>
<p>白居易在《钱塘湖春行》中写道：</p>
<blockquote>
<p>孤山寺北贾亭西，水面初平云脚低。几处早莺争暖树，谁家新燕啄春泥。乱花渐欲迷人眼，浅草才能没马蹄。</p>
</blockquote>
<p>2025 年 的春天，大模型的发展也正如诗句中所描述的意境那般：百花齐放，百家争鸣。在这个时候，大模型的评测也变得越来越重要。但是，如何才能确保在评测大模型时，能够得到正确、客观的结论呢？通过大量的实践、我们整理出了以下 7 条原则，希望能够帮助大家在评测大模型时，得到更加置信的结论。</p> 
      <a class="article-more-link" href="/2025/03/29/Seven-Principles-for-Large-Model-Evaluation/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AF%84%E6%B5%8B/" rel="tag">评测</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Reflections-on-the-LLMs-Technological-Evolution"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2024/12/01/Reflections-on-the-LLMs-Technological-Evolution/"
    >对大模型技术演化的思考</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/12/01/Reflections-on-the-LLMs-Technological-Evolution/" class="article-date">
  <time datetime="2024-12-01T10:58:56.000Z" itemprop="datePublished">2024-12-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%80%BB%E7%BB%93/">总结</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2024/12/01/Reflections-on-the-LLMs-Technological-Evolution/ai-generated.jpg" alt></p>
<p>最近的一段时间，为了满足自己对于 OpenAI 发布的 <em><a target="_blank" rel="noopener" href="https://openai.com/index/api-prompt-caching/">Prompt Caching in the API</a></em> 的强烈好奇心，对大模型的相关论文和技术做了非常多的梳理，包括了大模型的底层原理 Transformer 架构，到 GPT架构 的演变，到大模型的运行时推理、在线推理优化……</p>
<p>当我坐下来细细的回味这段解惑的时光，才发现畅游于大模型发展之路的沿途风景亦百花盛开。恰巧今年的工作大多和效率优化有关，再回顾一下自己的工作，发现这其中也存在很多相似的地方，例如：问题的量化与分析，优化方案的拆解，十字路口支出的技术信仰……</p> 
      <a class="article-more-link" href="/2024/12/01/Reflections-on-the-LLMs-Technological-Evolution/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%AE%9E%E8%B7%B5/" rel="tag">实践</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%80%9D%E8%80%83/" rel="tag">思考</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-insight-of-the-prompt-cache"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2024/11/23/insight-of-the-prompt-cache/"
    >Prompt Cache 究竟是什么？</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/11/23/insight-of-the-prompt-cache/" class="article-date">
  <time datetime="2024-11-23T22:17:05.000Z" itemprop="datePublished">2024-11-23</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2024/11/23/insight-of-the-prompt-cache/context_caching.png" alt></p>
<p>在介绍了 <a href="/2024/10/16/What-exactly-is-attention/">Transformer</a> 模型、<a href="/2024/10/31/From-Transformer-To-GPT/">GPT</a> 模型、<a href="/2024/11/16/The-LLMs-Runtime-Inference-and-KV-Cache/">大模型的运行时推理和 KV Cache</a> 后，我们终于越来越接近于最原始的目标：OpenAI 2024 年 10 月 1 日发布的 <a target="_blank" rel="noopener" href="https://openai.com/index/api-prompt-caching/">Prompt Caching in the API</a>。这篇文章，我们就来介绍一下 <code>Prompt Cache</code> 相关技术的发展并对 OpenAI 的 <code>Prompt Caching</code> 技术方案进行简单的分析。</p> 
      <a class="article-more-link" href="/2024/11/23/insight-of-the-prompt-cache/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM-Interface/" rel="tag">LLM Interface</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Prompt-Cache/" rel="tag">Prompt Cache</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-The-LLMs-Runtime-Inference-and-KV-Cache"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2024/11/16/The-LLMs-Runtime-Inference-and-KV-Cache/"
    >大模型的运行时推理和 KV Cache</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/11/16/The-LLMs-Runtime-Inference-and-KV-Cache/" class="article-date">
  <time datetime="2024-11-16T09:17:39.000Z" itemprop="datePublished">2024-11-16</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2024/11/16/The-LLMs-Runtime-Inference-and-KV-Cache/Prefill-and-decoding-phase-in-the-LLM-inference.png" alt></p>
<p>在 <a href="/2024/10/16/What-exactly-is-attention/">自注意力究竟是什么？</a> 和 <a href="/2024/10/31/From-Transformer-To-GPT/">从 Transformer 到 GPT</a> 中，我们介绍了 Transformer 架构的详细细节，并介绍了基于 Transformer 的 GPT 架构的细节。距离我们探究 <a target="_blank" rel="noopener" href="https://openai.com/index/api-prompt-caching/">Prompt Caching</a> 的原理又近了一步。就像 <code>程序</code> 和 <code>进程</code> 之间的区别一样，Prompt Caching 属于运行时的范畴，因此在探究 <a target="_blank" rel="noopener" href="https://openai.com/index/api-prompt-caching/">Prompt Caching</a> 的原理之前，我们还要继续了解大模型在部署和运行时推理方面的细节。</p> 
      <a class="article-more-link" href="/2024/11/16/The-LLMs-Runtime-Inference-and-KV-Cache/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/KV-Cache/" rel="tag">KV Cache</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Prefill/" rel="tag">Prefill</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Runtime-Inference/" rel="tag">Runtime Inference</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-From-Transformer-To-GPT"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2024/10/31/From-Transformer-To-GPT/"
    >从 Transformer 到 GPT</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/10/31/From-Transformer-To-GPT/" class="article-date">
  <time datetime="2024-10-31T18:19:48.000Z" itemprop="datePublished">2024-10-31</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2024/10/31/From-Transformer-To-GPT/1.png" alt><br>
在 <a href="/2024/10/16/What-exactly-is-attention/">自注意力究竟是什么？</a> 一文中，我们介绍了基于注意力机制的 Transformer 模型的基本原理和架构。</p>
<ul>
<li>2017年 6 月，谷歌机器翻译团队提出的机器翻译模型 Transformer 就像大语言模型的一颗种子一样，悄然落地生根，并迅速席卷了 AI 领域。</li>
<li>一年之后，2018 年 6 月，OpenAI 发布了基于 Transformer 架构的 GPT-1<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，虽然当时还存在一些局限性，例如当时还不能根据一个给定的标题来生成一篇新闻报道；但是，谁也没想到，就是这个框架，在 4 年之后成为了 AI 领域最炙手可热的模型。</li>
<li>4 个月后，2018 年 10 月，谷歌也发布了基于 Transformer 架构的 BERT 模型<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>，与 GPT-1 相比，BERT 在很多下游任务上表现出更强劲的性能，并且也刷新了多个榜单的记录。在很长一段时间里，BERT（及其变体）一直处于各类榜单的首位，是人们谈论的焦点。</li>
<li>直到 2022 年 3 月，OpenAI 发布了 GPT-3.5<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>，并基于 GPT-3.5 于当年的 11 月 30 日正式发布了面向消费用户的产品——ChatGPT，大模型再次引起了圈内、圈外的广泛讨论，开启了新一轮的大模型时代。</li>
</ul>
<p>这篇文章，我们就来详细的介绍一下传奇的 GPT 模型以及其原理，慢慢揭开 GPT 那神秘的面纱，也为后续对 <a target="_blank" rel="noopener" href="https://openai.com/index/api-prompt-caching/">Prompt Caching</a> 的讨论打下坚实的基础。</p> 
      <a class="article-more-link" href="/2024/10/31/From-Transformer-To-GPT/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPT/" rel="tag">GPT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-What-exactly-is-attention"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2024/10/16/What-exactly-is-attention/"
    >自注意力究竟是什么？</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/10/16/What-exactly-is-attention/" class="article-date">
  <time datetime="2024-10-16T17:19:11.000Z" itemprop="datePublished">2024-10-16</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/2024/10/16/What-exactly-is-attention/1.png" alt></p>
<p>最近的 1 年多以来，一直使用 文心一言、豆包、Kimi 等大模型来帮助自己提高各种场景的效率，但是一直没有对当前大模型的底层原理做深入了解。在编写 <a href="https://wangwei1237.github.io/LLM_in_Action/">Large Language Model in Action</a> 这本书的时候，我也曾说过：</p>
<blockquote>
<p>这是一本关于大语言模型实践的书籍，而不是一本深入研究大语言模型的运行原理和底层算法的书籍。</p>
</blockquote>
<p>但是，2024 年 10 月 1 日，OpenAI 发布了 <a target="_blank" rel="noopener" href="https://openai.com/index/api-prompt-caching/">Prompt Caching in the API</a> 以提升大语言模型 API 的性能。当听到这个消息的时候，我感到非常震惊，也非常兴奋，于是接下来的几天我总想搞明白这背后的原理是什么，这里的 <code>prompt caching</code> 又究竟是什么？</p>
<p>于是，我想，是时候需要深入了解一下当前大模型的起点——Transformer 模型，也是时候需要深入了解一下究竟什么是自注意力机制。</p> 
      <a class="article-more-link" href="/2024/10/16/What-exactly-is-attention/"
        >阅读更多...</a
      >
       
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Attention/" rel="tag">Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" rel="tag">自注意力机制</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2025
        <i class="ri-heart-fill heart_icon"></i> Wang Wei
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <!--<span id="busuanzi_container_page_pv">本文阅读量<span id="busuanzi_value_page_pv_"></span>次</span>
  <span class="division">|</span>
  -->
  <span id="busuanzi_container_site_uv"><i class="ri-user-3-fill"></i>本站访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span id="busuanzi_container_site_pv"><i class="ri-eye-fill"></i>本站浏览次数:<span id="busuanzi_value_site_pv"></span></span>
</span>
<script>
  
</script>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="17哥"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/books">图书</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/shares">分享</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/aboutme">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/weixin.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->
 
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css">
        <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js"></script>
        
            <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.js"></script>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/copy-tex.min.css">
        
    
 
<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "base" });
  }
</script>


    
    

  </div>
</body>

</html>